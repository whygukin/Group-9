---
layout: default
title: Status
---

## Summary of the Project (updated)
For this project, we would like to train an agent that will be able to play multi-agent poker games such as limited Texas hold'em and perform better than fixed policy agents (such as ones that always choose a random action or always chooses to call). We will take into account exploitability, round win-rate, and total earnings as metrics when considering an agent's performance. We will use Proximal Policy Optimization (PPO) to train the agent, because it doesn't require complete knowledge of a game state and is able to learn a strategy directly; improving policies iteratively. 

## Approach


## Evaluation


## Remaining Goals and Challenges



## Resources Used
We used OpenSpiel's universal_poker implementation as a starting point for our code, and are using its implemented algorithms as a library. We also referenced Stanford class project's paper (https://web.stanford.edu/class/aa228/reports/2018/final96.pdf) when considering our approach to our project. 
